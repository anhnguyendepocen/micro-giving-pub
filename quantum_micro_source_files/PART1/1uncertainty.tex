\chapter{Optimization and risk\index{risk}}
\label{1uncertainty}


\subsubsection{Motivating question\rm : What is the maximum amount $\$x$ you would pay to play a game (see Figure~\ref{fig:dtree_risk}) in which you flip a coin and get \$10 if it comes up heads and \$0 otherwise?}

\begin{figure}[H]
\begin{pspicture}(-4,-3)(16,3)
\pstree[treemode=R]{\TC*}
{
\TC*~[tnpos=r]{\fbox{\parbox{2.5cm}{Gain nothing,

lose nothing}}}
\taput{Refuse bet}
\TC*~[tnpos=r]{\fbox{\parbox{3cm}{50\%: Win $\$10-x$

50\%: Lose $\$x$}}}
\tbput{Take bet}
}
\end{pspicture}
\caption{A decision tree\index{decision tree} involving risk\index{risk}}
\label{fig:dtree_risk} % Figure~\ref{fig:dtree_risk}
\end{figure}

\noindent The important issue in this game is your attitude toward \textbf{risk}\index{risk}. People who are \textbf{risk\index{risk}-averse} buckle their seatbelts, buy insurance, and otherwise try to avoid risk\index{risk};  if you are this type of person you will be unwilling to pay \$5 or more to play this game. People who are \textbf{risk\index{risk}-loving} go skydiving, drive recklessly, or engage in other risk\index{risk}-seeking behaviors; if you are this type of person, you might be willing to pay more than \$5 to play this game. People who are \textbf{risk\index{risk}-neutral} are ambivalent about risk\index{risk}; if you are this type of person, you'd be willing to play this game for less than \$5, you'd avoid playing the game for more than \$5, and you would be indifferent about playing the game for exactly \$5.



\subsubsection{What's the deal with \$5?}

The deal is that \$5 is the \textbf{expected value\index{expected value|(}} of this game. Since you have a 50\% chance of getting \$10 and a 50\% chance of getting \$0, it makes some sense to say that \emph{on average} you'll get \$5. (Section~\ref{diversification} fleshes out this idea.) The concept of expected value provides a valuable perspective by condensing this risky situation into a single number. Note, however, that it conveys only one perspective; we'll soon see that different risky situations can have the same expected value, and to help differentiate them we'll introduce another concept called \textbf{variance\index{variance}}.

Mathematically, an expected value calculation weighs each possible outcome by its likelihood, giving more weight to more likely outcomes and less weight to less likely outcomes. In the game described above, the two possible outcomes are heads (H) and tails (T), so the expected value is
\begin{eqnarray*}
EV & = & \Pr(H) \cdot (\$10) + \Pr(T) \cdot (\$0) \\
& = & \frac{1}{2}\cdot (\$10) + \frac{1}{2} \cdot (\$0) \\
& = & \$5.
\end{eqnarray*}

More generally, we can write the formula for expected value as:
\[
\mbox{Expected Value}\ \ \  = \ \ \ \sum_{\mbox{Outcomes
\emph{i}}} \mbox{Probability(\emph{i})} \cdot
\mbox{Value(\emph{i})}.
\]
The Greek letter $\sum$ (``sigma'') is the mathematical notation for summation, e.g., $\displaystyle \sum_{y=1,2,3} y^2 = 1^2+2^2+3^2 = 14$.


\subsection*{Example: Fair and unfair bets}

A \textbf{fair bet\index{fair bet}} is a bet with an expected value of zero. Flipping a coin and winning $\$x$ if it comes up heads and losing $\$x$ if it comes up tails is a fair bet\index{fair bet}. If $x=5$, this is identical to the game above in which you pay me \$5 and then I pay you \$10 if the coin comes up heads and nothing if the coin comes up tails.

An \textbf{unfair bet\index{unfair bet}} is one with an expected value less than zero. If you go to a casino and play roulette, for example, you will see that the roulette wheel has 38 numbers (the numbers 1--36, plus 0 and 00). If you bet \$1 and guess the right number, your payoff is \$35; if you guess the wrong number, your payoff is $-\$1$, i.e., you lose your dollar. So if you bet \$1 on, say, number 8, then the expected value of the amount you'll get back is
\[
\Pr(8) \cdot (\$35) + \Pr(\mbox{Not $8$}) \cdot (-\$1) = \frac{1}{38}
\cdot (\$35) + \frac{37}{38} \cdot (-\$1) \approx -\$0.05.
\]
%
%Since it costs \$1 to play and your expected return is only \$0.95, the overall expected value of betting \$1 in roulette  is $-\$0.05$. So roulette is an unfair bet.



\section{Reducing risk with diversification}\label{diversification}\index{diversification}\index{risk!reducing with diversification}

Playing roulette \emph{once} is obviously a high-risk endeavor. (Presumably this is part of what makes it attractive to gamblers.)  It would therefore seem that owning a casino would also be a high-risk endeavor. \emph{But this is not the case.}

To see why, consider flipping a coin. Flip the coin once, and the actual percentage of heads will be either 0\% or 100\%. (In other words, the coin either comes up heads or it comes up tails.) Flip the coin twenty times, though, and the percentage of heads is likely to be pretty close to 50\%. (Specifically, the odds are better than 7 in 10 that the percentage of heads will be between 40\% and 60\%.) Flip the coin one hundred times and the odds are better than 95 in 100 that the percentage of heads will be between 40\% and 60\%. And flip the coin one thousand times and the odds are better than 998 in 1000 that the percentage of heads will be between 45\% and 55\%.

These results stem from the \textbf{law of large numbers}, a statistical result. In English, the law of large numbers says that if you flip a coin a large number of times, odds are that the proportion of heads will be close to 50\%. (Details \href{http://www.stat.berkeley.edu/~stark/Java/lln.htm}{online}\footnote{http://www.stat.berkeley.edu/\~{}stark/Java/lln.htm} and \href{http://www.ruf.rice.edu/~lane/stat_sim/binom_demo.html}{online}\footnote{http://www.ruf.rice.edu/\~{}lane/stat\_sim/binom\_demo.html}.) %http://www.ruf.rice.edu/~lane/stat_sim/normal_approx/index.html
%http://www.ruf.rice.edu/~lane/stat_sim/binom_demo.html
By extension, if each of a large number of coin flips pays \$10 for heads and \$0 for tails, odds are that you'll come close to averaging \$5 \emph{per coin flip}. It is no coincidence that \$5 also happens to be the expected value of this bet: repeat any bet a large number of times and odds are that the \emph{payout per bet} will be close to the expected value of the bet. This is the sense in which expected value can be thought of as the average payout of a bet.

\subsubsection{Risk and roulette}

To apply the law of large numbers to casinos, note that each gambler plays roulette only a few times, but that the casino plays roulette thousands of times each day. So the law of large numbers does not apply to individual gamblers, but it does apply to the casino. As a consequence, an individual gambler faces a great deal of risk, \emph{but the casino does not.} Since the expected value from betting \$1 on roulette is $-\$0.05$, odds are extremely good that the casino will gain about \$0.05 for each dollar wagered. As long as the mob doesn't get involved, then, running a casino is not necessarily any riskier than running, say, a photocopy shop. (Go \href{http://www.theonion.com/content/node/38825}{online}\footnote{http://www.theonion.com/content/node/38825} for an amusing story that illustrates this point.)

\subsubsection{Risk and the stock market}

Another application of the law of large numbers is in the stock market, where investors are often advised to \textbf{diversify} their portfolios. Compared to owning one or two stocks, investors can reduce risk by owning many stocks. (One way to do this is to own shares of an \textbf{index fund} that buys a little bit of everything.)

To see how \textbf{diversification} can reduce risk, consider a coin flip that pays \$100 if it comes up heads (H) and \$0 if it comes up tails (T). This situation has an expected value of \$50, but the risk is clear: either you win the whole \$100 or you win nothing.

\begin{comment}
The expected value from this coin flip is
\begin{eqnarray*}
EV & = & \Pr(H) \cdot (\$100) + \Pr(T) \cdot (\$0) \\
& = & \frac{1}{2}\cdot (\$100) + \frac{1}{2} \cdot (\$0) \\
& = & \$50.
\end{eqnarray*}
\end{comment}



Now consider flipping two coins, each paying \$50 for heads and \$0 for tails. The expected value in this situation is also \$50:
\begin{eqnarray*}
EV & = & \Pr(HH) \cdot (\$100) + \Pr(HT) \cdot (\$50) + \Pr(TH) \cdot (\$50) + \Pr(TT) \cdot (\$0) \\
& = & \frac{1}{4}\cdot (\$100) + \frac{1}{4}\cdot (\$50) + \frac{1}{4}\cdot (\$50) + \frac{1}{4} \cdot (\$0) = \$50.
\end{eqnarray*}
Compared to the one-coin flip, however, this two-coin flip is less risky: instead of always winning everything (\$100) or nothing (\$0), there is now a 50\% chance of winning something in between (\$50). A risk-averse individual should prefer the two-coin flip over the one-coin flip because it has less variability. (In the language of statistics, the two-coin flip has lower \textbf{variance}\index{variance}. In English, the two-coin flip means that you aren't putting all your eggs in one basket.)

The risk inherent in coin-flipping (or stock-picking) can be reduced even further by spreading the risk out even more, i.e., through more diversification. Flip twenty coins, each independently paying \$5 for heads and \$0 for tails, and the probability of getting an outcome that is ``in the middle" (say, between \$40 and \$60) is over 70\%. Flip one hundred coins, each independently paying \$1 for heads and \$0 for tails, and the odds of ending up between \$40 and \$60 is over 95\%. And flip one thousand coins, each independently paying \$0.10 for heads and \$0 for tails, and there is a 99.86\% chance that you will end up with an amount between \$45 and \$55. The expected value in all of these situations is \$50; what diversification does is reduce variance, so that with one thousand coin flips you are virtually guaranteed to end up with about \$50.

These coin-flipping results follow from the law of large numbers. But there is an important difference between coin-flipping and stock-picking: while the outcome of one coin flip has no influence on the outcome of the next one, stocks have a tendency to go up or down together, i.e., coin flips are \textbf{independent} while stock prices are \textbf{correlated}.) The law of large numbers applies to risks that are independent, so diversification of a stock market portfolio can reduce or eliminate risks that are not correlated, e.g., the risk that a particular company will be ruined by a corrupt CEO. But the law of large numbers does not apply when risks are correlated: diversification cannot reduce the risk of stock market crashes or other systemic risks that affect the entire economy.

A final point is that diversification is not always painless. It's easy to diversify if you have no preference for one stock over another---and we will see in Chapter~\ref{1transition} that this is a reasonable position to take. But if you have favorites then you have to balance risks against rewards: investing your life savings in your favorite stock may give you the highest expected payoff, but it also exposes you to a great deal of risk that you could reduce by diversifying. The optimal behavior in such situations is the subject of \textbf{portfolio selection theory}, a branch of economics whose development helped win James Tobin the 1981 Nobel Prize in Economics. When Tobin died in 2002, his  \emph{New York Times} obituary  (available  \href{http://query.nytimes.com/gst/fullpage.html?res=9801E6D61539F930A25750C0A9649C8B63}{online}\footnote{http://www.nytimes.com/}) included this story:
\begin{quote}
After he won the Nobel Prize, reporters asked him to explain the portfolio theory. When he tried to do so, one journalist interrupted, ``Oh, no, please explain it in lay language." So he described the theory of diversification by saying: ``You know, don't put your eggs in one basket." Headline writers around the world the next day created some version of ``Economist Wins Nobel for Saying, `Don't Put Eggs in One Basket'."
\end{quote}


% FIX: Do Akerlof's Market for Lemons!

\begin{CALCULUS}

\section{\emph{Math}: Indifference curves\index{expected value!and indifference curve|(}\index{indifference curve!and expected value|(}}

A more mathematical treatment of this material allows us introduce \textbf{indifference curves\index{indifference}}: we can ask when an individual is indifferent (i.e., has no preference) between one risky situation and another, or between a risky situation and a riskless one. For example, consider a coin-flipping bet in which someone wins \$100 for heads and \$0 for tails. Presumably this person would be indifferent between this bet and a symmetric bet in which he wins \$100 for tails and \$0 for heads. (If we refer to the first bet using the notation $(100,0)$, then the notation $(0,100)$ describes the second bet.) In fact, there are probably an infinite number of pairs $(x, y)$ such that he'd be indifferent between the bet $(100,0)$ and the bet $(x,y)$, i.e., a bet in which he wins $\$x$ for heads and $\$y$ for tails. If we graph all these pairs we get an indifference curve like the one in Figure~\ref{indifference:ev}. Using the utility-maximization idea described in Chapter~\ref{1intro}, we can say that all the points on this indifference curve have the same utility.



\psset{unit=.04cm}
\begin{figure}
\begin{center}
\begin{pspicture}(0,0)(110,110)
\rput[b](-8,108){$w_2$}
\rput[l](108,-5){$w_1$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](0,100){1}
\rput[l](4,100){$(0,100)$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](100,0){1}
\rput[b](105,4){$(100,0)$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](36.6,36.6){1}
\rput[lb](38,38){$(37,37)$}
\psplot{0}{100}{100 .02 mul 1 add .02 x mul 1 add div 1 sub .02 div} % add sub div mul exp log
\psaxes[labels=none, ticks=none, showorigin=false](110,110)
\end{pspicture}
\end{center}
\caption{An indifference curve for a coin-flipping bet}
\label{indifference:ev}
\end{figure}
\psset{unit=.5cm}


Of particular interest on this indifference curve is the point where $x=y$, because at this point the individual is not exposed to any uncertainty: regardless of the outcome of the coin flip, he gets the same amount. This amount is called the \textbf{certainty equivalent\index{certainty equivalent} wealth} corresponding to that indifference curve. For the curve in Figure~\ref{indifference:ev}, the certainty equivalent wealth is \$37, meaning that our individual is indifferent between getting \$37 \emph{with certainty} and facing the uncertain bets---such as $(100,0)$ and $(0,100)$---that are on the same indifference curve.

Note that the expected value of the bet $(100,0)$ is \$50, which is \emph{more} than the individual's certainty equivalent wealth. This implies that he is risk-averse; a risk-neutral person would have a certainty equivalent wealth of \$50 for the same original bet of $(100,0)$. The difference between the certainty equivalent wealth of a risk-averse person (in our example, \$37) and that of a risk-neutral person (\$50) makes it possible for insurance companies to provide a benefit to society and still make a profit. If an insurance company offers to ``buy" the bet $(100,0)$ from our risk-neutral individual for, say, \$40, he will accept the offer because he is indifferent between the bet $(100,0)$ and having \$37 with certainty, and having \$40 with certainty is better than having \$37 with certainty. (In the language of economics, accepting the offer of \$40 will put him on a higher indifference curve, i.e., one corresponding to a higher utility level.) What's in it for the insurance company? Well, they spend only \$40 to buy a bet that has an expected value of \$50; by buying many such bets, the insurance company has an extremely good chance of making a profit.

%One important implication here is that insurance companies only serve risk-averse individuals. You have to pay at least the expected value of your loss. So insurance protects against variability, not against expected loss.


\index{indifference curve!and expected value|)}\index{expected value!and indifference curve|)}

\end{CALCULUS}




\index{expected value|)}
\bigskip
\bigskip


\section*{Problems}

\noindent \textbf{Answers are in the endnotes beginning on page~\pageref{1uncertaintya}. If you're reading this online, click on the endnote number to navigate back and forth.}


\begin{enumerate}



\item You roll a six-sided die and win that amount (minimum \$1, maximum \$6). What is the expected value of this game?\endnote{\label{1uncertaintya}The expected value is $\frac{1}{6}(1) + \frac{1}{6}(2) + \frac{1}{6}(3) + \frac{1}{6}(4) + \frac{1}{6}(5) + \frac{1}{6}(6) = \frac{21}{6}.$}



\item With probability 1/3 you win \$99, with probability 2/3 you lose \$33. What is the expected value of this game?\endnote{The expected value is $\frac{1}{3}(99) + \frac{2}{3}(-33) = 11.$}




\item Imagine that you are taking a multiple-guess exam. There are five choices for each question; a correct answer is worth 1 point, and an incorrect answer is worth 0 points. You are on Problem \#23, and it just so happens that Problem \#23 is in Hungarian. (When you ask your teacher, she claims that the class learned Hungarian on Tuesday\ldots.)
    \begin{enumerate}
    \item You missed class on Tuesday, so you don't understand any  Hungarian. What is the expected value of guessing randomly on this problem?\endnote{The expected value of guessing randomly is $\frac{1}{5}(1) + \frac{4}{5}(0) = \frac{1}{5}.$}


    \item Now imagine that your teacher wants to discourage random guessing. To do this, she changes the scoring system, so that a blank answer is worth 0 points and an incorrect answer is worth $x$, e.g., $x=-\frac{1}{2}$. What should $x$ be in order to make random guessing among five answers a fair bet\index{fair bet} (i.e., one with an expected value of 0)?\endnote{If an incorrect answer is worth $x$, the expected value from guessing randomly is $\frac{1}{5}(1) + \frac{4}{5}(x) = \frac{1+4x}{5}.$ If the teacher wants this expected value to equal zero, she must set $x=-\frac{1}{4}.$}


   \item Is this ``fair bet" policy going to discourage test-takers who are risk\index{risk}-averse? %Yes   No (Circle one.)
What about those who are risk\index{risk}-loving?%Yes No (Circle one.)
\endnote{Since this makes random guessing a fair bet, it will discourage risk averse students but not risk loving students.}


   \item Your teacher ends up choosing $x=-\frac{1}{3}$, i.e., penalizing people 1/3rd of a point for marking an incorrect answer. How much Hungarian will you need to remember from your childhood in order to make guessing a better-than-fair bet\index{fair bet}? In other words, how many answers will you need to eliminate so that guessing among the remaining answers yields an expected value strictly greater than 0?\endnote{If you can't eliminate any answers, the expected value of guessing randomly is $\frac{1}{5}(1) + \frac{4}{5}\left(-\frac{1}{3}\right) = -\frac{1}{15}.$ If you can eliminate one answer, you have a 1 in 4 chance of getting the right answer if you guess randomly, so your expected value if you can eliminate one answer is $\frac{1}{4}(1) + \frac{3}{4}\left(-\frac{1}{3}\right) = 0.$ If you can eliminate two answers, you have a 1 in 3 chance of getting the right answer if you guess randomly, so your expected value if you can eliminate two answers is $\frac{1}{3}(1) + \frac{2}{3}\left(-\frac{1}{3}\right) = \frac{1}{9}.$ So you need to eliminate at least two answers in order to make random guessing yield an expected value greater than zero.}
    \end{enumerate}












\item Two businesses that involve lots of gambling are the casino business and the insurance business. Are these businesses particularly risky to get involved in? Explain why or why not.\endnote{No, they are not particularly risky. This is because of the \textbf{law of large numbers}, discussed in Section~\ref{diversification}. The individual bettor plays roulette only a few times, and so faces a lot of risk\index{risk}. The casino plays roulette thousands of times each day, and so has a very good idea of what the overall outcome will be; since each \$1 wager has an expected payoff of only \$.95, it can expect to gain about \$.05 for every dollar wagered.

Similarly, although insurance companies have no idea whether an individual driver is going to get into an accident this year, or whether an individual person is going to die this year, or whether an individual home is going to burn to the ground this year, the law of large numbers usually gives them a very good idea of the percentage of accidents or the percentage of deaths or the percentage of fires to expect from the hundreds of thousands of cars, lives, and homes they cover.

So casinos or insurance companies are not necessarily any riskier as business endeavors than, say, running a photocopy shop.}







\item \emph{Fun/Challenge.} (The Monty Hall Problem)\index{Hall, Monty}\index{Monty Hall Problem} This problem gets its name from the TV game show \emph{Let's Make A Deal}, hosted by Monty Hall. The scenario is this: Monty shows you three closed doors. Behind one of these doors is a new car. Behind the other two doors are goats (or some other ``non-prize"). Monty asks you to choose a door, but after you do he doesn't show you what's behind the door you chose. Instead, he opens one of the \emph{other} doors, revealing a goat, and then offers you the opportunity to switch to the remaining unopened door. [As an example, say you originally pick Door \#1. Monty opens up Door \#2, revealing a goat, and then offers you the opportunity to switch from Door \#1 to Door \#3.] (You can go \href{http://en.wikipedia.org/wiki/Monty_Hall_problem}{online}\footnote{http://en.wikipedia.org/wiki/Monty\_Hall\_problem} for a fun computer simulation of the Monty Hall problem, with accompanying discussion.) What should you do?\endnote{You should switch: your odds of winning will increase from $\frac{1}{3}$ to $\frac{2}{3}$. A more extreme example may help provide some intuition behind this result: assume that there are 100 doors, only one of which leads to a car; after you pick a door, Monty opens up 98 of the other doors to reveal goats and then offers you the opportunity to switch to the remaining unopened door. Doing so will increase your odds of winning from $\frac{1}{100}$ to $\frac{99}{100}$.}



\item Howard Raiffa's\index{Raiffa!Howard} book \emph{Decision Analysis: Introductory Lectures on Choices Under Uncertainty} (1997) does an extensive analysis of variations on the following basic problem.

There are 1,000 urns. Eight hundred of them are of type $U_1$; each of these contain four red balls and six black balls. The remaining two hundred are of type $U_2$; each of these contain nine red balls and one black ball. One of these 1,000 urns is chosen at random and placed in front of you; you cannot identify its type or see the balls inside it. Which one of the following options maximizes your expected value, and what is that expected value?
\begin{description}
\item[Option 1] Guess that the urn is of type $U_1$. If you are correct, you win \$40.00. Otherwise, you lose \$20.00.
\item[Option 2] Guess that the urn is of type $U_2$. If you are correct, you win \$100.00. Otherwise, you lose \$5.00.
\item[Option 3] Refuse to play the game.\endnote{Your expected value from Option 1 is .8(40)+.2(-20) = 28. Your expected value from Option 2 is .8(-5)+.2(100) = 16. Your expected value from Option 3 is 0. So Option 1 maximizes your expected value.}
\end{description}










\item \emph{Challenge.} (One variation) Prior to choosing one of the three options described above, you can conduct \emph{at most one} of the following investigations. (Note that you can also choose not to conduct any of these.) What strategy maximized your expected value, and what is that expected value?
\begin{description}
\item[Investigation 1] For a payment of \$8.00, you can draw a single ball at random from the urn. \item[Investigation 2] For a payment of \$12.00, you can draw two balls from the urn. \item[Investigation 3] For a payment of \$9.00, you can draw a single ball from the urn, and then (after looking at it) decide whether or not you want to pay \$4.50 to draw another ball. (Whether or not you want to replace the first ball before drawing the second is up to you.)\endnote{This is a difficult problem. For more information on it, read Raiffa's book or do some research on Bayes's Rule.}
\end{description}










\item You're a bidder in an auction\index{auction} for an antique vase. If you lose the auction, you get nothing. If you win the auction, assume that your gain is the difference between the maximum amount you'd be willing to pay for the vase---say, \$100---and the actual amount that you end up paying. (So if you pay \$80, your gain is \$20.)

    \begin{enumerate}

    \item In a \textbf{first-price sealed bid auction\index{auction}}, you write down your bid $b$ on a piece of paper and submit it to the auctioneer in a sealed envelope. After all the bids have been submitted, the auctioneer opens the envelopes and finds the highest bidder. That bidder gets the item, and pays a price equal to their bid. If the probability of winning with a bid of $b$ is $\Pr(b)$, write down an expected value calculation for this auction\index{auction}.%
    \endnote{Your expected value from bidding $b$ in either type of auction is
\[
\mbox{Prob($b$ wins)}\cdot \mbox{Value($b$ wins)} + \mbox{Prob($b$
loses)}\cdot \mbox{Value($b$ loses)}.
\]
In a first-price auction, Value($b$ wins)$=100-b$ and Value($b$
loses)$=0$; so your expected value is
\[
\mbox{Prob($b$ wins)}\cdot (100-b) + 0.
\]}


    \item In a \textbf{second-price sealed bid auction\index{auction}}, everything's the same except that the winning bidder (the person with the highest bid) pays a price equal to the \emph{second-highest} bid. Write down an expected value calculation for this auction\index{auction} if the probability of winning with a bid of $b$ is $\Pr(b)$ and the highest bid \emph{less} than $b$ is $c$.\endnote{In a second-price auction, Value($b$ wins)$=100-c$, where $c$ is
the highest bid less than $b$, and Value($b$ loses)$=0$. So your
expected value is
\[
\mbox{Prob($b$ wins)}\cdot (100-c) + 0.
\]}


   \item \emph{Challenge.} From your answers above, can you figure out what kind of strategy (i.e., what bid $b$) will maximize your expected value in the different auctions? In particular: should you bid your true value, $b=\$100$, or should you bid more or less than your true value? (We'll study auctions\index{auction} more in Chapter~\ref{2auctions}.)\endnote{Chapter~\ref{2auctions} discusses this in more detail.}
    \end{enumerate}











\item Although many services are provided on a fee-up-front basis (for example, you pay before you enter the movie theater), some services are provided on the ``honor system". Parking meters are one example: instead of paying someone when you park your car, you are ``on your honor" to put money in the meter. (Some cities also do this with their public transportation systems.) Of course, it isn't just a matter of honor: there are also enforcement officers---``meter maids"---who show up from time to time and penalize rule-breakers. So:

    \begin{enumerate}

    \item What is the expected cost of ``risking it" by putting nothing in the meter if there's an 80\% chance that you'll get away with it (so your cost will be \$0) and a 20\% chance that you'll get a \$20 ticket?\endnote{The expected cost is $(.80)(0)+(.20)(\$20) = \$4$.}


\begin{comment}
    \item If the ``safe option" is to put \$3 in the meter, can you say for sure whether a risk-neutral individual will risk it or to play it safe? (Circle one: Risk it\ \ \ \ Play it safe\ \ \ \ Can't say for sure) Can you say for sure what a risk-averse individual will do? (Circle one: Risk it\ \ \ \ Play it safe\ \ \ \ Can't say for sure) What about a risk-loving individual? (Circle one: Risk it\ \ \ \ Play it safe\ \ \ \ Can't say for sure) (\emph{Hint:} Recall that risk-averse individuals avoid fair bets, that risk-loving individuals accept fair bets, and that risk-neutral individuals are indifferent towards fair bets.)\endnote{Since the safe option has a lower expected cost (\$3 instead of \$4), a risk-neutral individual will choose to play it safe. Since risk-averse individuals don't take fair bets, they will also play it safe. But we can't say for sure about risk-loving individuals because it depends on how much they like risk.}
\end{comment}


    \item Imagine that the city managers want to save money by cutting in half the number of enforcement officers (so that the chance of getting a ticket is only 10\%). Can you suggest a way to do this without increasing the attractiveness of cheating?\endnote{The city could double the amount of the ticket from \$20 to \$40. This would mean that the expected value of risking it is still \$4: $(.90)(0) + (.10)(\$40) = \$4$.}
    \end{enumerate}


\end{enumerate}


















\begin{comment}


\begin{EXAM}
\section*{Problems}

\input{part1/qa1uncertainty}
\end{EXAM}


\begin{CALCULUS}

\section{\emph{Math}: Expected Value and Indifference Curves\index{expected value!and indifference curve|(}\index{indifference curve!and expected value|(}}

A more mathematical treatment of this material allows us to do two additional things, both related to expected utility. First, we can see that there is a relationship between the second derivative of the utility function and the individual's attitude toward risk\index{risk}. This result follows from comparing $E\left( u(w)\right)$ and $u\left( E(w)\right)$ using a theorem called Jensen's Inequality. The end result is this:
\begin{itemize}
\item For a concave utility function ($u''<0$, e.g., $u(w)=\sqrt{w}$) Jensen's Inequality says that $E\left[u(w)\right]<u\left[ E(w)\right]$, so this individual is risk\index{risk} averse.

\item For a convex utility function ($u''>0$, e.g., $u(w)=w^2$) Jensen's Inequality says that $E\left[ u(w)\right]>u\left[E(w)\right]$, so this individual is risk\index{risk} loving.

\item For a linear utility function ($u''=0$, e.g., $u(w)=w$) Jensen's Inequality says that $E\left[ u(w)\right]=u\left[E(w)\right]$, so this individual is risk\index{risk} neutral.
\end{itemize}
%

The second additional topic is \textbf{indifference\index{indifference}}: we can ask when an individual is indifferent (i.e., has no preference) between one risky situation and another, or between a risky situation and a riskless one. For example, consider an individual who has a 40\% chance of having wealth $w_1$ and a 60\% chance of having wealth $w_2$. Her utility function is $u(w)=\sqrt{w}$, and she maximizes expected utility, $E[u(w)]=.4\sqrt{w_1}+.6\sqrt{w_2}$. If $(w_1,w_2)=(0,100)$, her expected utility is $.4\sqrt{0}+.6\sqrt{100}=.6(10)=6$. If $(w_1,w_2)=(225,0)$ her expected utility is $.4\sqrt{225}+.6\sqrt{0}=.4(15)=6$. Since her expected utility is the same in both cases, she is indifferent between them.

In fact, there are an infinite number of pairs $(w_1,w_2)$ that satisfy $E[u(w)]=6$. Figure~\ref{indifference:ev} shows a graph of these points, which is a called an \textbf{indifference curve}. In this case, the indifference curve corresponds to an expected utility of 6: every point $(w_1,w_2)$ on the curve gives our individual an expected utility of 6, and so she is indifferent between any two of them.

\psset{unit=.5cm}
\begin{figure}
\begin{center}
\begin{pspicture}(0,0)(21,10)
\rput[b](0,10.2){$w_2$}
\rput[l](21.2,0){$w_1$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](0,8.888){.1}
\rput[l](.4,8.888){$(0,100)$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](20,0){.1}
\rput[b](20,.4){$(225,0)$}
\pscircle[fillstyle=solid, linecolor=black, fillcolor=black](3.2,3.2){.1}
\rput[lb](3.3,3.3){$(36,36)$}
\psplot{0}{20}{60 11.25 x mul sqrt 4 mul sub 2 exp 36 div 11.25 div} % add sub div mul exp log
\psaxes[labels=none, ticks=none, showorigin=false](21,10)
\end{pspicture}
\end{center}
\caption{An indifference curve corresponding to expected utility of 6}
\label{indifference:ev}
\end{figure}
\psset{unit=.5cm}


In particular, note that the point $(w_1,w_2)=(36,36)$ lies on this indifference curve: $E[u(w)]=.4\sqrt{36}+.6\sqrt{36}=\sqrt{36}=6$. In this situation our individual has a 40\% chance of having wealth $w_1=36$ and a 60\% chance of having wealth $w_2=36$, i.e., a 100\% chance of having wealth $w=36.$ This amount of wealth is therefore called the \textbf{certainty equivalent\index{certainty equivalent}} wealth corresponding to an expected utility of 6.

\subsubsection*{Example: \rm An individual with initial wealth of 400 has a 10\% chance of getting in an accident. If she gets in an accident, she will lose 300; if she doesn't, she loses nothing. She maximizes expected utility, and her utility function is $u(w)=\sqrt{w}$.
\begin{enumerate}
\item What is the expected amount of money she will lose?
%
\item What is her expected utility?
%
\item What is her certainty equivalent wealth?
%
\item What is the maximum amount she would pay for \textbf{full insurance\index{insurance!full}}? (Note: Full insurance totally insulates her from the risk\index{risk} of an accident. If she pays $x$ for full insurance, her wealth will be $400-x$ regardless of whether or not she gets in an accident.)
%
\item How much more is she willing to pay for insurance than would a risk\index{risk}-neutral individual? (This amount is called her \textbf{risk premium}.)
\end{enumerate}}

Answer: Her expected loss is $.1(300)+.9(0)=30.$ Her expected utility is $.1\sqrt{400-300}+.9\sqrt{400-0}=19.$ Her certainty equivalent wealth is $w$ such that having wealth of $w$ with probability $1$ gives her an expected utility of $19$: $\sqrt{w}=19\Longrightarrow w=361.$

The maximum amount she would pay for full insurance is $400-361=39.$ This payment makes her indifferent between buying insurance, in which case $E[u(w)]=.1\sqrt{400-39}+.9\sqrt{400-39}=\sqrt{361}=19$, and not buying insurance, in which case $E[u(w)]=.1\sqrt{400-300}+.9\sqrt{400-0}=19$. To look at it another way: if she is offered insurance for some amount $x<39$, she will take it because $u(400 - x)>19$; if she is offered insurance for some amount $x>39$, she will refuse it because $u(400-x)<19$; if she is offered insurance for $x=39$ she will be indifferent between accepting and rejecting.

Finally, we can calculate her risk premium\index{risk!premium} in this situation. She is willing to pay up to 39 for full insurance, but her expected loss is only 30; since a risk\index{risk} neutral individual would pay a maximum of 30 for full insurance, the difference ($39-30=9$) is her risk premium\index{risk!premium}. Its source is risk\index{risk} aversion: our individual is willing to pay money to avoid a fair bet\index{fair bet}. We can see that she is risk\index{risk} averse by calculating the second derivative of the utility function $u(w)=\sqrt{w}$: $u'(w)=\frac{1}{2}w^{-\frac{1}{2}}$, so $u''(w)=-\frac{1}{4}w^{-\frac{3}{2}}<0.$\index{indifference curve!and expected value|)}\index{expected value!and indifference curve|)}

\end{CALCULUS}
\end{comment} 